{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nelsonufomba/Intro-to-CS50-Lab-1-Python/blob/main/ECE_57000_COSAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rBGRe-ScAOCw"
      },
      "outputs": [],
      "source": [
        "import argparse\n",
        "import pickle\n",
        "import time\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import TensorDataset\n",
        "import pandas as pd\n",
        "import csv\n",
        "import operator\n",
        "import datetime\n",
        "import os\n",
        "from torch.utils.data import Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "81Wjo13TAuer",
        "outputId": "28371eca-c713-4b58-b6b6-00c39e50de8a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Can I can use GPU now? -- True\n",
            "input is on cpu\n",
            "model parameters are on [device(type='cpu'), device(type='cpu'), device(type='cpu'), device(type='cpu')]\n",
            "output is on cpu\n",
            "input is on cuda:0\n",
            "model parameters are on [device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0)]\n",
            "output is on cuda:0\n"
          ]
        }
      ],
      "source": [
        "print(f'Can I can use GPU now? -- {torch.cuda.is_available()}')\n",
        "\n",
        "rand_tensor = torch.rand(5,2)\n",
        "simple_model = nn.Sequential(nn.Linear(2,10), nn.ReLU(), nn.Linear(10,1))\n",
        "print(f'input is on {rand_tensor.device}')\n",
        "print(f'model parameters are on {[param.device for param in simple_model.parameters()]}')\n",
        "print(f'output is on {simple_model(rand_tensor).device}')\n",
        "\n",
        "# device = torch.device('cuda')\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "# ----------- <Your code> ---------------\n",
        "# Move rand_tensor and model onto the GPU device\n",
        "rand_tensor = rand_tensor.to(device)\n",
        "simple_model = simple_model.to(device)\n",
        "\n",
        "# --------- <End your code> -------------\n",
        "print(f'input is on {rand_tensor.device}')\n",
        "print(f'model parameters are on {[param.device for param in simple_model.parameters()]}')\n",
        "print(f'output is on {simple_model(rand_tensor).device}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cvGqCJeAA-5s",
        "outputId": "8fe7e064-ab27-46d3-baf6-14438e2f6566"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HbE61hlKqWgc",
        "outputId": "42f345b4-9502-4f88-b68c-31fc9528a942"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-6-d722d8bd75e0>:6: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  data = pd.read_csv(file_path, header=None, names=headers)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   session_id                 timestamp    item_id category\n",
            "0           1  2014-04-07T10:51:09.277Z  214536502        0\n",
            "1           1  2014-04-07T10:54:09.868Z  214536500        0\n",
            "2           1  2014-04-07T10:54:46.998Z  214536506        0\n",
            "3           1  2014-04-07T10:57:00.306Z  214577561        0\n",
            "4           2  2014-04-07T13:56:37.614Z  214662742        0\n",
            "-- Starting @ 2024-11-07 01:26:27.599515\n"
          ]
        }
      ],
      "source": [
        "# Define the path to your data file\n",
        "file_path = '/content/drive/MyDrive/ECE57000Project/archive-3/yoochoose-clicks.dat'\n",
        "\n",
        "# Load the data into a DataFrame and add headers if not already present\n",
        "headers = ['session_id', 'timestamp', 'item_id', 'category']\n",
        "data = pd.read_csv(file_path, header=None, names=headers)\n",
        "#data.to_csv(file_path, index=False)  # Save with headers if needed\n",
        "\n",
        "# Dataset and file path setup\n",
        "dataset_option = 'yoochoose'  # Choose 'sample' or 'yoochoose'\n",
        "dataset_file = file_path\n",
        "\n",
        "# Display the first 5 rows\n",
        "print(data.head(5))\n",
        "\n",
        "print(\"-- Starting @ %s\" % datetime.datetime.now())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aTpOeNvLtmdr",
        "outputId": "a4787634-fa18-4dc3-e4bc-1278f4a219c0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  session_id                 timestamp    item_id category\n",
            "0          1  2014-04-07T10:51:09.277Z  214536502        0\n",
            "1          1  2014-04-07T10:54:09.868Z  214536500        0\n",
            "2          1  2014-04-07T10:54:46.998Z  214536506        0\n",
            "3          1  2014-04-07T10:57:00.306Z  214577561        0\n",
            "4          2  2014-04-07T13:56:37.614Z  214662742        0\n"
          ]
        }
      ],
      "source": [
        "# Drop the first three rows\n",
        "data = data.drop(index=[0]).reset_index(drop=True)\n",
        "data.to_csv(file_path, index=False)  # Save with headers if needed\n",
        "\n",
        "# Display the first 5 rows after dropping\n",
        "print(data.head(5))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iRKuRj1RsNDg",
        "outputId": "35767dd9-4668-47e8-cedc-2a72a606d4bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data read successfully at 2024-11-07 00:02:41.958426\n",
            "Training sessions: 7974678, Testing sessions: 15340\n",
            "Sample train sequences: [['214594678', '214820231'], ['214687685', '214705119'], ['214716982', '214716982', '214716982']]\n",
            "Sample test sequences: [['214847885', '214857260', '214602729', '214851120'], ['214858847', '214857257', '214858850', '214859034', '214858850'], ['214717115', '214829820', '214645005']]\n",
            "Data saved successfully.\n",
            "Processing Complete.\n"
          ]
        }
      ],
      "source": [
        "import csv\n",
        "import os\n",
        "from pathlib import Path\n",
        "import pickle\n",
        "from datetime import datetime\n",
        "from collections import defaultdict\n",
        "import operator\n",
        "\n",
        "# Constants\n",
        "MIN_ITEM_OCCURRENCES = 5\n",
        "MIN_SESSION_LENGTH = 2\n",
        "SECONDS_PER_DAY = 86400\n",
        "YOOCHOOSE_SPLIT_DAYS = 1\n",
        "DIGINETICA_SPLIT_DAYS = 7\n",
        "\n",
        "def read_dataset(file_path):\n",
        "    sess_clicks = defaultdict(list)\n",
        "    sess_date = {}\n",
        "\n",
        "    try:\n",
        "        with open(file_path, \"r\") as f:\n",
        "            reader = csv.reader(f, delimiter=',')\n",
        "            next(reader)  # Skip header row\n",
        "\n",
        "            for row in reader:\n",
        "                session_id, timestamp, item_id = row[0], row[1], row[2]\n",
        "\n",
        "                # Convert timestamp to UNIX timestamp\n",
        "                unix_time = parse_timestamp(timestamp)\n",
        "\n",
        "                sess_clicks[session_id].append(item_id)\n",
        "                sess_date[session_id] = unix_time\n",
        "\n",
        "        print(f\"Data read successfully at {datetime.now()}\")\n",
        "        return sess_clicks, sess_date\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading dataset: {e}\")\n",
        "        return None, None\n",
        "\n",
        "def parse_timestamp(timestamp):\n",
        "    if timestamp.endswith('Z'):\n",
        "        timestamp = timestamp[:-1]  # Remove 'Z' for parsing\n",
        "    return datetime.strptime(timestamp, '%Y-%m-%dT%H:%M:%S.%f').timestamp()\n",
        "\n",
        "def filter_sessions(sess_clicks, sess_date):\n",
        "    # Filter sessions with only one click\n",
        "    sess_clicks = {s: clicks for s, clicks in sess_clicks.items() if len(clicks) > 1}\n",
        "    sess_date = {s: sess_date[s] for s in sess_clicks}\n",
        "\n",
        "    # Count item occurrences\n",
        "    iid_counts = defaultdict(int)\n",
        "    for seq in sess_clicks.values():\n",
        "        for iid in seq:\n",
        "            iid_counts[iid] += 1\n",
        "\n",
        "    # Filter items appearing fewer than MIN_ITEM_OCCURRENCES times\n",
        "    sess_clicks = {\n",
        "        s: [i for i in seq if iid_counts[i] >= MIN_ITEM_OCCURRENCES]\n",
        "        for s, seq in sess_clicks.items()\n",
        "        if len([i for i in seq if iid_counts[i] >= MIN_ITEM_OCCURRENCES]) >= MIN_SESSION_LENGTH\n",
        "    }\n",
        "\n",
        "    return sess_clicks, sess_date\n",
        "\n",
        "def split_data(sess_clicks, sess_date, dataset_option):\n",
        "    split_days = YOOCHOOSE_SPLIT_DAYS if dataset_option == 'yoochoose' else DIGINETICA_SPLIT_DAYS\n",
        "    split_date = max(sess_date.values()) - (SECONDS_PER_DAY * split_days)\n",
        "\n",
        "    tra_sess = sorted([(s, d) for s, d in sess_date.items() if d < split_date], key=operator.itemgetter(1))\n",
        "    tes_sess = sorted([(s, d) for s, d in sess_date.items() if d >= split_date], key=operator.itemgetter(1))\n",
        "\n",
        "    print(f\"Training sessions: {len(tra_sess)}, Testing sessions: {len(tes_sess)}\")\n",
        "    return tra_sess, tes_sess\n",
        "\n",
        "def obtain_sequences(sessions, sess_clicks, item_dict=None):\n",
        "    seq_ids, seq_dates, seq_items = [], [], []\n",
        "    item_count = 1 if item_dict is None else len(item_dict) + 1\n",
        "    sess_clicks = defaultdict(list, sess_clicks)  # Convert to defaultdict\n",
        "\n",
        "    for session_id, date in sessions:\n",
        "        outseq = []\n",
        "        for item in sess_clicks[session_id]:  # Now safe to access\n",
        "            if item_dict is not None:\n",
        "                if item not in item_dict:\n",
        "                    item_dict[item] = item_count\n",
        "                    item_count += 1\n",
        "                outseq.append(item_dict[item])\n",
        "            else:\n",
        "                outseq.append(item)\n",
        "\n",
        "        if len(outseq) >= MIN_SESSION_LENGTH:\n",
        "            seq_ids.append(session_id)\n",
        "            seq_dates.append(date)\n",
        "            seq_items.append(outseq)\n",
        "\n",
        "    return seq_ids, seq_dates, seq_items, item_dict\n",
        "\n",
        "def save_processed_data(output_dir, train_data, test_data):\n",
        "    output_path = Path(output_dir)\n",
        "    output_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    with open(output_path / 'train.txt', 'wb') as f:\n",
        "        pickle.dump(train_data, f)\n",
        "    with open(output_path / 'test.txt', 'wb') as f:\n",
        "        pickle.dump(test_data, f)\n",
        "\n",
        "    print(\"Data saved successfully.\")\n",
        "\n",
        "def main(dataset_file, dataset_option):\n",
        "    # Read and process data\n",
        "    sess_clicks, sess_date = read_dataset(dataset_file)\n",
        "    if sess_clicks is None or sess_date is None:\n",
        "        return\n",
        "\n",
        "    sess_clicks, sess_date = filter_sessions(sess_clicks, sess_date)\n",
        "    tra_sess, tes_sess = split_data(sess_clicks, sess_date, dataset_option)\n",
        "\n",
        "    # Obtain sequences\n",
        "    train_ids, train_dates, train_seqs, item_dict = obtain_sequences(tra_sess, sess_clicks)\n",
        "    test_ids, test_dates, test_seqs, _ = obtain_sequences(tes_sess, sess_clicks, item_dict)\n",
        "\n",
        "    # Confirm sample sequences\n",
        "    print(\"Sample train sequences:\", train_seqs[:3])\n",
        "    print(\"Sample test sequences:\", test_seqs[:3])\n",
        "\n",
        "    # Save processed data\n",
        "    output_dir = f'/content/drive/MyDrive/ECE57000Project/{(\"yoochoose1_64\" if dataset_option == \"yoochoose\" else \"diginetica\")}'\n",
        "    save_processed_data(output_dir, (train_seqs, train_ids), (test_seqs, test_ids))\n",
        "\n",
        "    print(\"Processing Complete.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    dataset_file = \"/content/drive/MyDrive/ECE57000Project/archive-3/yoochoose-clicks.dat\"  # Replace with actual path\n",
        "    dataset_option = \"yoochoose\"  # or \"diginetica\"\n",
        "    main(dataset_file, dataset_option)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load processed data\n",
        "output_dir = '/content/drive/MyDrive/ECE57000Project/yoochoose1_64'  # Adjust based on your output directory\n",
        "with open(f'{output_dir}/train.txt', 'rb') as f:\n",
        "    train_data = pickle.load(f)\n",
        "\n",
        "with open(f'{output_dir}/test.txt', 'rb') as f:\n",
        "    test_data = pickle.load(f)\n",
        "\n",
        "train_seqs, train_ids = train_data\n",
        "test_seqs, test_ids = test_data\n",
        "\n",
        "print(\"Loaded training sequences:\", len(train_seqs))\n",
        "print(\"Loaded testing sequences:\", len(test_seqs))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J6Ao75GTWdC0",
        "outputId": "acdffcf0-8aad-4484-9fcd-178168ab0f7b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded training sequences: 7966257\n",
            "Loaded testing sequences: 15324\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a custom dataset class\n",
        "class SessionDataset(Dataset):\n",
        "    def __init__(self, sequences, targets):\n",
        "        self.sequences = sequences\n",
        "        self.targets = targets # Store targets\n",
        "    # 1. Create an item-to-ID mapping:\n",
        "\n",
        "        self.item2id = {}\n",
        "        self.id2item = {}\n",
        "        for seq in self.sequences:\n",
        "            for item in seq:\n",
        "                if item not in self.item2id:\n",
        "                    self.item2id[item] = len(self.item2id)\n",
        "                    self.id2item[len(self.id2item)] = item\n",
        "\n",
        "        # 2. Use the mapping to convert item sequences to ID sequences:\n",
        "        self.sequences = [[self.item2id[item] for item in seq] for seq in self.sequences]\n",
        "        #self.targets = self.sequences  # Or load them separately if needed\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sequences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        items = torch.tensor(self.sequences[idx], dtype=torch.long)\n",
        "        target = torch.tensor(self.targets[idx], dtype=torch.float)  # Adjust for binary or multi-class targets\n",
        "        return items, targets\n",
        "\n",
        "# Assuming 'sequences' is your list of item sequences\n",
        "import random\n",
        "\n",
        "# Generate random targets (example - adjust as needed)\n",
        "targets = [[random.randint(0, 1) for _ in seq] for seq in train_seqs]\n",
        "\n",
        "# Create datasets\n",
        "train_dataset = SessionDataset(train_seqs, targets)\n",
        "test_dataset = SessionDataset(test_seqs, targets)\n",
        "\n",
        "# Create data loaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n"
      ],
      "metadata": {
        "id": "CAP1nOM8WWEM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "def load_in_data_batches(train_path, test_path, batch_size):\n",
        "    # Load training data\n",
        "    train_data = pd.read_csv(train_path)  # Adjust based on how you saved your data\n",
        "    test_data = pd.read_csv(test_path)    # Same as above\n",
        "\n",
        "    # Assuming train_data and test_data have 'item_sequence' and 'target' columns\n",
        "    train_sequences = train_data['item_sequence'].apply(eval).tolist()  # Convert string representation of lists back to lists\n",
        "    train_targets = train_data['target'].tolist()\n",
        "\n",
        "    test_sequences = test_data['item_sequence'].apply(eval).tolist()\n",
        "    test_targets = test_data['target'].tolist()\n",
        "\n",
        "    # Create datasets\n",
        "    train_dataset = SessionDataset(train_sequences, train_targets)\n",
        "    test_dataset = SessionDataset(test_sequences, test_targets)\n",
        "\n",
        "    # Create data loaders\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    return train_loader, test_loader"
      ],
      "metadata": {
        "id": "KNgJp74TYsRG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class CollaborativeSelfAttentionNetwork(nn.Module):\n",
        "    def __init__(self, opt, n_node, n_user):\n",
        "        super(CollaborativeSelfAttentionNetwork, self).__init__()\n",
        "        self.hidden_size = opt.hiddenSize\n",
        "        self.n_node = n_node\n",
        "        self.n_user = n_user\n",
        "        self.batch_size = opt.batchSize\n",
        "        self.n_heads = opt.nhead\n",
        "        self.n_layers = opt.layer\n",
        "        self.dropout_rate = opt.dropout\n",
        "        self.alpha = opt.alpha\n",
        "\n",
        "        # Embeddings\n",
        "        self.item_embedding = nn.Embedding(self.n_node, self.hidden_size)\n",
        "        self.session_embedding = nn.Embedding(self.n_user, self.hidden_size)  # Using n_user as proxy for n_sessions\n",
        "\n",
        "        # Multi-head self-attention with layer normalization\n",
        "        self.self_attention = nn.MultiheadAttention(self.hidden_size, self.n_heads, dropout=self.dropout_rate)\n",
        "\n",
        "        # Feed-forward network with dropout\n",
        "        self.feed_forward = nn.Sequential(\n",
        "            nn.Linear(self.hidden_size, self.hidden_size * 4),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(self.dropout_rate),  # Added dropout here for better regularization\n",
        "            nn.Linear(self.hidden_size * 4, self.hidden_size)\n",
        "        )\n",
        "\n",
        "        # Layer normalization applied after feed-forward network\n",
        "        self.layer_norm1 = nn.LayerNorm(self.hidden_size)\n",
        "        self.layer_norm2 = nn.LayerNorm(self.hidden_size)\n",
        "\n",
        "        # Prediction layer with bias initialization\n",
        "        self.prediction = nn.Linear(self.hidden_size * 2, self.n_node)\n",
        "        nn.init.xavier_uniform_(self.prediction.weight)  # Xavier initialization for better convergence\n",
        "\n",
        "        # Loss function and optimizer setup\n",
        "        self.loss_function = nn.BCEWithLogitsLoss()\n",
        "        self.optimizer = torch.optim.Adam(self.parameters(), lr=opt.lr, weight_decay=opt.l2)\n",
        "\n",
        "        # Learning rate scheduler\n",
        "        self.scheduler = torch.optim.lr_scheduler.StepLR(self.optimizer, step_size=opt.lr_dc_step, gamma=opt.lr_dc)\n",
        "\n",
        "    def forward(self, items, session_ids, neighborhood_sessions, neighborhood_weights):\n",
        "        # Collaborative Item Representation\n",
        "        item_emb = self.item_embedding(items)\n",
        "\n",
        "        # Improved neighborhood representation calculation\n",
        "        neighborhood_emb = self.session_embedding(neighborhood_sessions)\n",
        "        complementary_feature = torch.sum(neighborhood_emb * neighborhood_weights.unsqueeze(-1), dim=1)\n",
        "\n",
        "        collaborative_item_rep = item_emb + (self.alpha * complementary_feature)\n",
        "\n",
        "        # Self-attention layers with residual connections and normalization\n",
        "        for _ in range(self.n_layers):\n",
        "            attn_output, _ = self.self_attention(collaborative_item_rep.transpose(0, 1), collaborative_item_rep.transpose(0, 1), collaborative_item_rep.transpose(0, 1))\n",
        "            attn_output = attn_output.transpose(0, 1)\n",
        "            collaborative_item_rep = self.layer_norm1(collaborative_item_rep + attn_output)  # Residual connection\n",
        "\n",
        "            ff_output = self.feed_forward(collaborative_item_rep)\n",
        "            collaborative_item_rep = self.layer_norm2(collaborative_item_rep + ff_output)  # Residual connection\n",
        "\n",
        "        # Final session representation combining last item representation and session embedding\n",
        "        session_emb = self.session_embedding(session_ids)\n",
        "\n",
        "        final_rep = torch.cat([collaborative_item_rep[:, -1, :], session_emb], dim=1)\n",
        "\n",
        "        # Prediction scores calculation\n",
        "        scores = self.prediction(final_rep)\n",
        "\n",
        "        return scores\n",
        "\n",
        "    def compute_loss(self, scores, targets):\n",
        "        return self.loss_function(scores, targets)"
      ],
      "metadata": {
        "id": "aYU6Ps9Obzho"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_neighborhood_data(items, neighborhood_data):\n",
        "    batch_size, seq_len = items.size()\n",
        "    return torch.randint(0, 1000, (batch_size, seq_len, 10)), torch.rand(batch_size, seq_len, 10)"
      ],
      "metadata": {
        "id": "Y_p0zVSuYBSW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_test(model, train_loader, test_loader, user_ids, neighborhood_data, device):\n",
        "    print('Start training: ', time.strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "\n",
        "    for batch_idx, (items, targets) in enumerate(train_loader):\n",
        "        model.optimizer.zero_grad()\n",
        "\n",
        "        batch_size = items.size(0)\n",
        "        session_ids = user_ids[batch_idx * batch_size : (batch_idx + 1) * batch_size]\n",
        "\n",
        "        # Get neighborhood data for this batch\n",
        "        neighborhood_sessions, neighborhood_weights = get_neighborhood_data(items, neighborhood_data)\n",
        "\n",
        "        items = items.to(device)\n",
        "        targets = targets.to(device)\n",
        "        session_ids = torch.LongTensor(session_ids).to(device)\n",
        "        neighborhood_sessions = neighborhood_sessions.to(device)\n",
        "        neighborhood_weights = neighborhood_weights.to(device)\n",
        "\n",
        "        scores = model(items, session_ids, neighborhood_sessions, neighborhood_weights)\n",
        "        loss = model.compute_loss(scores, targets)\n",
        "\n",
        "        loss.backward()\n",
        "        model.optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        if batch_idx % 200 == 0:\n",
        "            print(f'[{batch_idx}/{len(train_loader)}] Loss: {loss.item():.4f}')\n",
        "\n",
        "    print(f'\\tTotal Loss:\\t{total_loss:.3f}')\n",
        "\n",
        "    print('Start predicting: ', time.strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
        "    model.eval()\n",
        "    hit, mrr = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (items, targets) in enumerate(test_loader):\n",
        "            batch_size = items.size(0)\n",
        "            session_ids = user_ids[batch_idx * batch_size : (batch_idx + 1) * batch_size]\n",
        "\n",
        "            # Get neighborhood data for this batch\n",
        "            neighborhood_sessions, neighborhood_weights = get_neighborhood_data(items, neighborhood_data)\n",
        "\n",
        "            items = items.to(device)\n",
        "            targets = targets.to(device)\n",
        "            session_ids = torch.LongTensor(session_ids).to(device)\n",
        "            neighborhood_sessions = neighborhood_sessions.to(device)\n",
        "            neighborhood_weights = neighborhood_weights.to(device)\n",
        "\n",
        "            scores = model(items, session_ids, neighborhood_sessions, neighborhood_weights)\n",
        "\n",
        "            sub_scores = scores.topk(20)[1]\n",
        "            sub_scores = sub_scores.cpu().numpy()\n",
        "            targets = targets.cpu().numpy()\n",
        "\n",
        "            for score, target in zip(sub_scores, targets):\n",
        "                hit.append(np.isin(target - 1, score))\n",
        "                if target - 1 in score:\n",
        "                    mrr.append(1 / (np.where(score == target - 1)[0][0] + 1))\n",
        "                else:\n",
        "                    mrr.append(0)\n",
        "\n",
        "    hit_rate = np.mean(hit) * 100\n",
        "    mrr_rate = np.mean(mrr) * 100\n",
        "    model.scheduler.step()\n",
        "    return hit_rate, mrr_rate\n",
        "\n"
      ],
      "metadata": {
        "id": "gIbjlClzNLk-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "opt = type('', (), {})()\n",
        "opt.hiddenSize = 100\n",
        "opt.batchSize = 32\n",
        "opt.nhead = 2\n",
        "opt.layer = 2\n",
        "opt.dropout = 0.1\n",
        "opt.alpha = 0.5\n",
        "opt.lr = 0.001\n",
        "opt.l2 = 1e-5\n",
        "opt.lr_dc_step = 3\n",
        "opt.lr_dc = 0.1\n",
        "opt.n_epochs = 10\n",
        "\n",
        "\n",
        "# Model initialization\n",
        "n_node = 37484\n",
        "n_user = 1000\n",
        "model = CollaborativeSelfAttentionNetwork(opt, n_node, n_user)\n",
        "#model = trans_to_cuda(model)  # Make sure this function moves the model to GPU if available\n",
        "\n",
        "# Generate user IDs (adjust if you have actual user IDs)\n",
        "user_ids = np.random.randint(0, n_user, size=(len(train_loader.dataset),))\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(opt.n_epochs):\n",
        "    model.train()  # Set model to training mode\n",
        "    total_loss = 0\n",
        "\n",
        "    for batch in train_loader:\n",
        "        items, session_ids, neighborhood_sessions, neighborhood_weights, targets = batch\n",
        "\n",
        "        # Move data to GPU if applicable\n",
        "        items = items.to(device)\n",
        "        session_ids = session_ids.to(device)\n",
        "        neighborhood_sessions = neighborhood_sessions.to(device)\n",
        "        neighborhood_weights = neighborhood_weights.to(device)\n",
        "        targets = targets.to(device)\n",
        "\n",
        "        # Initialize optimizer\n",
        "        optimizer = torch.optim.Adam(model.parameters(), lr=opt.lr, weight_decay=opt.l2)\n",
        "\n",
        "        optimizer.zero_grad()  # Zero out gradients\n",
        "\n",
        "        # Forward pass\n",
        "        scores = model(items, session_ids, neighborhood_sessions, neighborhood_weights)\n",
        "\n",
        "        # Compute loss (adjust according to your output layer)\n",
        "        loss = model.compute_loss(scores, targets)\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "    print(f'Epoch {epoch + 1}/{opt.n_epochs}, Loss: {avg_loss:.4f}')\n",
        "\n",
        "    # Evaluate on test set after each epoch\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "    hit_rate, mrr_rate = train_test(model, train_loader, test_loader, user_ids, neighborhood_data, device)\n",
        "    print(f'Epoch {epoch + 1}/{opt.n_epochs}, Hit Rate: {hit_rate:.2f}%, MRR: {mrr_rate:.2f}%')\n",
        "\n",
        "print(\"Training completed.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "TOiiI0ejG28Z",
        "outputId": "420b0029-dda3-49d7-ad2a-883a30f6ee68"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "stack expects each tensor to be equal size, but got [6] at entry 0 and [2] at entry 1",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-40-533a6c66e152>\u001b[0m in \u001b[0;36m<cell line: 26>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mtotal_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m         \u001b[0mitems\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msession_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneighborhood_sessions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneighborhood_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    699\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    700\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 701\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    702\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m             if (\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    755\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    756\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 757\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    758\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    759\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    396\u001b[0m         \u001b[0;34m>>\u001b[0m\u001b[0;34m>\u001b[0m \u001b[0mdefault_collate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Handle `CustomType` automatically\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    397\u001b[0m     \"\"\"\n\u001b[0;32m--> 398\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcollate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdefault_collate_fn_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m             return [\n\u001b[0m\u001b[1;32m    212\u001b[0m                 \u001b[0mcollate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtransposed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    210\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m             return [\n\u001b[0;32m--> 212\u001b[0;31m                 \u001b[0mcollate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    213\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtransposed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m             ]  # Backwards compatibility.\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    153\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcollate_fn_map\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0melem_type\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0melem_type\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcollate_type\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mcollate_tensor_fn\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    270\u001b[0m         \u001b[0mstorage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_typed_storage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_new_shared\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 272\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    273\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: stack expects each tensor to be equal size, but got [6] at entry 0 and [2] at entry 1"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip freeze > requirements.txt"
      ],
      "metadata": {
        "id": "PGKZWZSUdhnM"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "provenance": [],
      "authorship_tag": "ABX9TyMljmK0BMA74JMEqJZrVgvb",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}